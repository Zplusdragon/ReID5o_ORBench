# ReID5o_ORBench
We investigate a new challenging problem called **Omni Multi-modal Person Re-identification** (OM-ReID), which aims to achieve effective retrieval with varying multi-modal queries. 

To address dataset scarcity, we construct **ORBench**, the first high-quality multi-modal dataset comprising 1,000 unique identities across five modalities: RGB, infrared, color pencil, sketch, and textual description. It could serve as an ideal platform for follow-up investigations in OM-ReID. 

Moreover, we propose **ReID5o**, a novel multi-modal learning framework for person ReID. It enables synergistic fusion and cross-modal alignment of arbitrary modality combinations in a single model, with a unified encoding and multi-expert routing mechanism proposed.

More details can be found at our paper [ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model](https://arxiv.org/abs/2312.03441).
